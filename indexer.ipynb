{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maggiebarclay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "This is being skipped,likely because of an unrecognized letter/character\n",
      "463\n",
      "463\n"
     ]
    }
   ],
   "source": [
    "# get all of the info from html files\n",
    "# two lists, one of the names one with the descriptions\n",
    "\n",
    "characterList = []\n",
    "descriptionsList = []\n",
    "\n",
    "for filename in os.listdir(\"project_crawler/project_crawler/htmlFiles\"):\n",
    "    try:\n",
    "        #print(\"reading file: \" + str(filename))\n",
    "        with open(\"project_crawler/project_crawler/htmlFiles/\" + str(filename), 'r') as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            character = soup.title.getText().split()[0]\n",
    "            characterList.append(character)\n",
    "\n",
    "            # appending separate, not sure if getting all description parts yet or just the first\n",
    "            descList = []\n",
    "            descList.append(soup.find_all('p')[1].getText().replace('\\n',' '))\n",
    "            descList.append(soup.find_all('p')[2].getText().replace('\\n',' '))\n",
    "            descList.append(soup.find_all('p')[3].getText().replace('\\n',' '))\n",
    "\n",
    "            description = \"\".join(descList)\n",
    "            descriptionsList.append(description)\n",
    "\n",
    "    except:\n",
    "        print(filename)\n",
    "        print(\"This is being skipped,likely because of an unrecognized letter/character\")\n",
    "        pass\n",
    "\n",
    "\n",
    "#print(characterList)\n",
    "print(len(characterList))\n",
    "#print(descriptionsList)\n",
    "print(len(descriptionsList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bam is a jock deer villager in the Animal Crossing series. He first appeared in Animal Crossing: New Leaf, and he was one of the first villagers announced for the release of the game. He has appeared in all subsequent games. As of Animal Crossing: New Horizons, he is the only jock deer in the series. His name may be derived from Bambi, a male deer who is the main character of the novel Bambi, a Life in the Woods and its film adaptation. In New Horizons, Bam has the play hobby and can run around with his arms extended at any time, rather than only outside Resident Services during a scripted activity with a group of villagers. Bam is a blue deer. He has lighter blue muzzle and rings around his eyes, orange irises with brown pupils, dark gray freckles, and brown antlers and brown hooves. The insides of his ears are yellow. '"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptionsList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizerMethod(docString: str):\n",
    "  noPunctString = (docString.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "  tokenizedDoc = word_tokenize(noPunctString)\n",
    "  return tokenizedDoc\n",
    "\n",
    "tokenizedDocList = []\n",
    "\n",
    "for item in descriptionsList:\n",
    "  tokenizedDocList.append(tokenizerMethod(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bam', 'is', 'a', 'jock', 'deer', 'villager', 'in', 'the', 'animal', 'crossing', 'series', 'he', 'first', 'appeared', 'in', 'animal', 'crossing', 'new', 'leaf', 'and', 'he', 'was', 'one', 'of', 'the', 'first', 'villagers', 'announced', 'for', 'the', 'release', 'of', 'the', 'game', 'he', 'has', 'appeared', 'in', 'all', 'subsequent', 'games', 'as', 'of', 'animal', 'crossing', 'new', 'horizons', 'he', 'is', 'the', 'only', 'jock', 'deer', 'in', 'the', 'series', 'his', 'name', 'may', 'be', 'derived', 'from', 'bambi', 'a', 'male', 'deer', 'who', 'is', 'the', 'main', 'character', 'of', 'the', 'novel', 'bambi', 'a', 'life', 'in', 'the', 'woods', 'and', 'its', 'film', 'adaptation', 'in', 'new', 'horizons', 'bam', 'has', 'the', 'play', 'hobby', 'and', 'can', 'run', 'around', 'with', 'his', 'arms', 'extended', 'at', 'any', 'time', 'rather', 'than', 'only', 'outside', 'resident', 'services', 'during', 'a', 'scripted', 'activity', 'with', 'a', 'group', 'of', 'villagers', 'bam', 'is', 'a', 'blue', 'deer', 'he', 'has', 'lighter', 'blue', 'muzzle', 'and', 'rings', 'around', 'his', 'eyes', 'orange', 'irises', 'with', 'brown', 'pupils', 'dark', 'gray', 'freckles', 'and', 'brown', 'antlers', 'and', 'brown', 'hooves', 'the', 'insides', 'of', 'his', 'ears', 'are', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizedDocList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "\n",
    "def calculateDF(tokenizedDocList: list):\n",
    "  invInd = {}\n",
    "  index = -1\n",
    "  for doc in tokenizedDocList:\n",
    "    index += 1\n",
    "    for word in list(set(doc)):\n",
    "      word = (word.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "      word = \"$\" + word + \"$\"\n",
    "      if word in invInd:\n",
    "        currentDF = invInd[word]\n",
    "        currentDF += 1\n",
    "        invInd[word] = currentDF\n",
    "      else: \n",
    "        invInd[word] = 1\n",
    "  return invInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInvInd(tokenizedDocList: list):\n",
    "  invInd = {}\n",
    "  index = -1\n",
    "  #N is the number of docs \n",
    "  N = len(tokenizedDocList)\n",
    "  dfIndx = calculateDF(tokenizedDocList)\n",
    "\n",
    "  for doc in tokenizedDocList:\n",
    "    index += 1\n",
    "    for word in list(set(doc)):\n",
    "      formatWord = '$' + word + '$'\n",
    "      word = (word.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "      #tf is how many times term appears in the document / len of doc <- going off of book so not dividing by length\n",
    "      tf = doc.count(word)\n",
    "      df = dfIndx[formatWord]\n",
    "      idf = np.log(N/(df))\n",
    "      tfidf = (tf*idf)\n",
    "\n",
    "      if formatWord in invInd:    \n",
    "        currList = invInd[formatWord]\n",
    "        currList.append(((str(index),  str(characterList[index]), tfidf)))\n",
    "        invInd[formatWord] = currList\n",
    "        \n",
    "      else:   \n",
    "        invInd[formatWord] = [(str(index), str(characterList[index]), tfidf)]\n",
    "\n",
    "  return invInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('57', 'Lucky', 5.919346447476577), ('75', 'Goldie', 8.879019671214865), ('89', 'Bea', 11.838692894953153), ('97', 'Butch', 14.798366118691442), ('100', 'Portia', 14.798366118691442), ('115', 'Jeremiah', 2.9596732237382883), ('122', 'Holden', 2.9596732237382883), ('151', 'Cobb', 2.9596732237382883), ('187', 'Bones', 8.879019671214865), ('198', 'Marcel', 5.919346447476577), ('203', 'Shep', 11.838692894953153), ('221', 'Vivian', 2.9596732237382883), ('225', 'Mac', 8.879019671214865), ('229', 'Cherry', 8.879019671214865), ('237', 'Cookie', 11.838692894953153), ('293', 'Walker', 5.919346447476577), ('295', 'Daisy', 2.9596732237382883), ('349', 'Maddie', 8.879019671214865), ('351', 'Chai', 2.9596732237382883), ('403', 'Biskit', 8.879019671214865), ('412', 'Frett', 5.919346447476577), ('413', 'Kyle', 5.919346447476577), ('429', 'Marty', 2.9596732237382883), ('452', 'Benjamin', 5.919346447476577)]\n"
     ]
    }
   ],
   "source": [
    "invIndex = makeInvInd(tokenizedDocList)  \n",
    "#print(invIndex[\"dog\"])\n",
    "\n",
    "# the method below is a user friendly way to lookup, just put in a word as usual and it will do the $ addition for you\n",
    "def lookupInDocs(term):\n",
    "  term = \"$\" + term + \"$\"\n",
    "  return invIndex[term]\n",
    "\n",
    "print(lookupInDocs(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryVector(docsList, query):\n",
    "  query = query.split()\n",
    "  queryIndx = {}\n",
    "  dfIndx = calculateDF(docsList)\n",
    "\n",
    "  # idf = log (docs in corpus / docs with term in them)\n",
    "  N = len(docsList)\n",
    "\n",
    "  for term in query:\n",
    "    term = \"$\" + term + \"$\"\n",
    "    df = dfIndx[term]\n",
    "    idf = np.log(N/(df))\n",
    "\n",
    "    if term in queryIndx:    \n",
    "        pass\n",
    "    else:   \n",
    "        queryIndx[term] = idf\n",
    "  return queryIndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$blue$': 0.9119803803730329, '$dog$': 2.9596732237382883}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryIndex = queryVector(tokenizedDocList, \"blue dog\")\n",
    "queryIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Butch', 0.4171269329196854), ('Portia', 0.4171269329196854), ('Bea', 0.3337015463357483), ('Shep', 0.3337015463357483), ('Cookie', 0.3337015463357483), ('Goldie', 0.25027615975181117), ('Bones', 0.25027615975181117), ('Mac', 0.25027615975181117), ('Cherry', 0.25027615975181117), ('Maddie', 0.25027615975181117)]\n"
     ]
    }
   ],
   "source": [
    "def cosine(query, index):\n",
    "    scores = {}\n",
    "\n",
    "    for query_term, query_weight in query.items():\n",
    "        for charNum, character, doc_weight in index[query_term]:\n",
    "          \n",
    "          if character not in scores: \n",
    "            scores[character] = query_weight * doc_weight\n",
    "            #print(charNum, character, doc_weight)\n",
    "          else:\n",
    "            scores[character] += query_weight * doc_weight  \n",
    "            #print(charNum, character, doc_weight)\n",
    "\n",
    "    finalScores = {}\n",
    "    for character in scores.keys():\n",
    "      finalScores[character] = (float(scores[character]) / (len(tokenizedDocList[int(charNum)])))\n",
    "\n",
    "    return sorted(finalScores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "results = cosine(queryIndex, invIndex)\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing block of docs with indecies: 0 through 154\n",
      "writing block of docs with indecies: 154 through 308\n",
      "writing block of docs with indecies: 308 through 462\n"
     ]
    }
   ],
   "source": [
    "numberOfBatches = 3\n",
    "\n",
    "def processDocs(docsList, numberOfBatches):\n",
    "  start = 0\n",
    "  blockSize = int(len(docsList)/numberOfBatches)\n",
    "\n",
    "  listOfPairs = []\n",
    "\n",
    "  for block in range(numberOfBatches):\n",
    "    for i in range(start, start+blockSize):\n",
    "      tokenizedDoc = docsList[i]\n",
    "      for term in tokenizedDoc:\n",
    "        listOfPairs.append((term, i))\n",
    "\n",
    "    print(\"writing block of docs with indecies: \" + str(start) + \" through \" +  str(start+blockSize))\n",
    "    listfile = open('listPickle','wb')\n",
    "    pickle.dump(listOfPairs,listfile)\n",
    "    listfile.close()\n",
    "\n",
    "    start += blockSize\n",
    "\n",
    "processDocs(tokenizedDocList, numberOfBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  pickle_in = open('listPickle','rb')\n",
    "  unpickedLists = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(invInd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
