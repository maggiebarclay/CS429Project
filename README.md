# CS429Project

The proposed system is comprised of three main components:
SCRAPY CRAWLER

-> downloads web documents in html format
-> project name: project_crawler
-> spider name: webby_spider
-> in project directory, scrapy crawl webby
-> DONT RUN THE CRAWLER, DOCUMENTS ALREADY SAVED LOCALLY
    -> You can run it again but you'll just get the exact same html files which are already in this repo




(base) maggiebarclay@Maggies-MacBook-Pro CS429Project % export FLASK_APP=processor 